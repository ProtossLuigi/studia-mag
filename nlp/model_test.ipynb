{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader, stopwords\n",
    "import fasttext\n",
    "# from convokit import Corpus, download\n",
    "# from xgboost.sklearn import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import re\n",
    "# from autosklearn.regression import AutoSklearnRegressor\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\proto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\proto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\proto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_oanc = PlaintextCorpusReader('./OANC-GrAF', '.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_m1 = fasttext.train_unsupervised('corpus_general_plain.txt', model='cbow')\n",
    "# full_m1.save_model('full_m1.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_m2 = fasttext.train_unsupervised('corpus_general_plain.txt', model='skipgram')\n",
    "# full_m2.save_model('full_m2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_m1 = fasttext.train_unsupervised('corpus_specialized_plain.txt', model='cbow')\n",
    "# sample_m1.save_model('sample_m1.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_m2 = fasttext.train_unsupervised('corpus_specialized_plain.txt', model='skipgram')\n",
    "# sample_m2.save_model('sample_m2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('corpus_specialized_annotated.csv').sample(50000)\n",
    "data['text'] = data['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['text'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('.*\\w.*')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if r.match(token) and token.lower() not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['tokens'].apply(clean_tokens)\n",
    "data = data.drop(data[data['tokens'].str.len() == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_length(vectors, dst_len=50):\n",
    "    if len(vectors) < dst_len:\n",
    "        return vectors + [[0] * 100] * (dst_len - len(vectors))\n",
    "    elif len(vectors) > dst_len:\n",
    "        return vectors[:dst_len]\n",
    "    else:\n",
    "        return vectors\n",
    "\n",
    "# def get_vectorizer(model):\n",
    "#     def get_text_repr(tokens):\n",
    "#         if tokens:\n",
    "#             vectors = [model.get_word_vector(word) for word in tokens[:50]]\n",
    "#         else:\n",
    "#             vectors = []\n",
    "#         return np.concatenate(equalize_length(vectors))\n",
    "#     return get_text_repr\n",
    "\n",
    "def get_vectorizer(model):\n",
    "    def get_text_repr(tokens):\n",
    "        return torch.tensor(np.stack([model.get_word_vector(word) for word in tokens]))\n",
    "    return get_text_repr\n",
    "\n",
    "# def prepare_data(data, model):\n",
    "#     vectorizer = get_vectorizer(model)\n",
    "#     X = np.stack(data['text'].apply(vectorizer))\n",
    "#     y = np.array(data['meta.score'])\n",
    "#     mask = np.random.rand(len(X)) < 0.8\n",
    "#     X_train = X[mask]\n",
    "#     X_test = X[~mask]\n",
    "#     y_train = y[mask]\n",
    "#     y_test = y[~mask]\n",
    "#     return X_train, y_train, X_test, y_test\n",
    "\n",
    "def prepare_data(data, model):\n",
    "    vectorizer = get_vectorizer(model)\n",
    "    X = data['text'].apply(vectorizer)\n",
    "    y = data['meta.score']\n",
    "    mask = np.random.rand(len(X)) < 0.8\n",
    "    X_train = list(X[mask])\n",
    "    X_test = list(X[~mask])\n",
    "    y_train = list(y[mask])\n",
    "    y_test = list(y[~mask])\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_path in ['full_m1.bin', 'full_m2.bin', 'sample_m1.bin', 'sample_m2.bin']:\n",
    "#     vectorizer = fasttext.FastText.load_model(model_path)\n",
    "#     X_train, y_train, X_test, y_test = prepare_data(data, vectorizer)\n",
    "#     regressor = AutoSklearnRegressor(memory_limit=10240)\n",
    "#     regressor.fit(X_train, y_train, X_test, y_test)\n",
    "#     y_pred = regressor.predict(X_test)\n",
    "#     print(model_path)\n",
    "#     print('\\tmse:\\t', mean_squared_error(y_test, y_pred))\n",
    "#     print('\\tr2:\\t', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class score_predictor(nn.Module):\n",
    "\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Conv1d(100, 100, kernel_size=5),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(100, 100, kernel_size=5),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(100, 100, kernel_size=5),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(8800, 4400),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(4400, 2200),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(2200, 1100),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1100, 1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y) -> None:\n",
    "        super().__init__()\n",
    "        self.data = list(zip(X, y))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(100, 100, 3, batch_first=True)\n",
    "        self.reg = nn.Linear(100, 1)\n",
    "    \n",
    "    def forward(self, x, x_lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.rnn(packed)\n",
    "        unpacked, unpacked_len = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        indices = Variable(torch.LongTensor(np.array(unpacked_len) - 1).view(-1, 1)\n",
    "                                                                       .expand(unpacked.size(0), unpacked.size(2))\n",
    "                                                                       .unsqueeze(1)\n",
    "                                                                       .to(device))\n",
    "        last_encoded_states = unpacked.gather(dim=1, index=indices).squeeze(dim=1)\n",
    "        return self.reg(last_encoded_states)\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    X_batch, y_batch = zip(*batch)\n",
    "    lengths = [sample.shape[0] for sample in X_batch]\n",
    "    X_batch = nn.utils.rnn.pad_sequence(X_batch, batch_first=True)\n",
    "    return X_batch, torch.tensor(y_batch, dtype=torch.float).unsqueeze(1), lengths\n",
    "\n",
    "def to_dl(X, y):\n",
    "    ds = MyDataset(X, y)\n",
    "    dl = DataLoader(ds, 128, shuffle=True, collate_fn=my_collate, pin_memory=True)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dl):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for X_batch, y_batch, lengths in dl:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_true.append(y_batch)\n",
    "        y_pred.append(model(X_batch, lengths).cpu())\n",
    "    y_pred = torch.concat(y_pred)\n",
    "    y_true = torch.concat(y_true)\n",
    "    return mean_squared_error(y_true, y_pred), r2_score(y_true, y_pred)\n",
    "\n",
    "def fit(model, loss_fn, optimizer, train_dl, val_dl, epochs=50, show_metrics=True):\n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, y_batch, lengths in train_dl:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(X_batch, lengths)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if show_metrics:\n",
    "            with torch.no_grad():\n",
    "                train_mse, train_r2 = validate(model, train_dl)\n",
    "                val_mse, val_r2 = validate(model, val_dl)\n",
    "                print(f'Epoch: {epoch}\\ttrain: MSE = {train_mse} R2 = {train_r2}\\tval: MSE = {val_mse} R2 = {val_r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " full_m1.bin \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\proto\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\proto\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([63])) that is different to the input size (torch.Size([63, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11620/967762019.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11620/3608600890.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, loss_fn, optimizer, train_dl, val_dl, epochs, show_metrics)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshow_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[0mtrain_mse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_r2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mval_mse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_r2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "for model_path in ['full_m1.bin', 'full_m2.bin', 'sample_m1.bin', 'sample_m2.bin']:\n",
    "    vectorizer = fasttext.FastText.load_model(model_path)\n",
    "    X_train, y_train, X_test, y_test = prepare_data(data, vectorizer)\n",
    "    train_dl = to_dl(X_train, y_train)\n",
    "    test_dl = to_dl(X_test, y_test)\n",
    "    model = MyNet().to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    print('\\n', model_path, '\\n')\n",
    "    fit(model, loss_fn, optimizer, train_dl, test_dl)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae9258babb9dc6183a521d7a445c874d7696eb0fb582154c3a2ca8b33699b65d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
